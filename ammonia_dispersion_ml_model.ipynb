{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fffdc828",
   "metadata": {},
   "source": [
    "# Ammonia Dispersion Prediction using Deep Learning\n",
    "\n",
    "This notebook implements a deep learning model to predict ammonia concentration and threat zones based on environmental conditions.\n",
    "\n",
    "## Features:\n",
    "- Predicts indoor and outdoor ammonia concentrations (ppm)\n",
    "- Predicts threat zone distances (Red, Orange, Yellow zones in miles)\n",
    "- Uses neural networks with hyperparameter optimization\n",
    "- Comprehensive visualization and performance metrics\n",
    "\n",
    "## Authors:\n",
    "Research conducted at University of Engineering and Technology (UET)\n",
    "\n",
    "## License:\n",
    "MIT License - See LICENSE file for details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a2f5d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Install required packages if needed:\n",
    "```bash\n",
    "pip install numpy pandas matplotlib scikit-learn tensorflow keras-tuner tabulate openpyxl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tabulate\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf3b53",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "**Note:** Make sure `unique_points.xlsx` is in the same directory as this notebook.\n",
    "\n",
    "### Input Features:\n",
    "- Air Temperature (°C)\n",
    "- Wind Speed (m/s)\n",
    "- Hole Diameter (mm)\n",
    "\n",
    "### Target Variables:\n",
    "- Indoor Concentration (ppm)\n",
    "- Outdoor Concentration (ppm)\n",
    "- Red Zone (miles)\n",
    "- Orange Zone (miles)\n",
    "- Yellow Zone (miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa231389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data = pd.read_excel(\"unique_points.xlsx\", sheet_name=\"Sheet1\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset Shape: {data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(data.head())\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a878953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = data[[\"Air Temperature (°C)\", \"Wind Speed (m/s)\", \"Hole Diameter (mm)\"]]\n",
    "\n",
    "# Targets (includes threat zones)\n",
    "y = data[['Indoor Concentration (ppm)', 'Outdoor Concentraton (ppm)',\n",
    "          'Red Zone (miles) ', 'Orange Zone (miles) ', 'Yellow Zone (miles)']]\n",
    "\n",
    "print(f\"Features Shape: {X.shape}\")\n",
    "print(f\"Targets Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2416495",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Scale features and targets using MinMaxScaler to normalize values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Features and Targets\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "print(\"Scaling completed!\")\n",
    "print(f\"Scaled Features range: [{X_scaled.min():.3f}, {X_scaled.max():.3f}]\")\n",
    "print(f\"Scaled Targets range: [{y_scaled.min():.3f}, {y_scaled.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a38ea6",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split\n",
    "\n",
    "Split data into training (80%) and testing (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd8419",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning (Optional)\n",
    "\n",
    "This section uses Keras Tuner to find the best hyperparameters. \n",
    "\n",
    "**Note:** Set `run_hyperparameter_tuning = True` to run tuning (may take considerable time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93677fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning using Keras Tuner\n",
    "run_hyperparameter_tuning = False  # Set to True to run tuning\n",
    "\n",
    "if run_hyperparameter_tuning:\n",
    "    def build_model(hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "        for i in range(hp.Int(\"num_layers\", 2, 3)):\n",
    "            model.add(layers.Dense(\n",
    "                units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "                activation='relu'\n",
    "            ))\n",
    "            model.add(layers.Dropout(rate=hp.Float(f'dropout_{i}', 0.0, 0.4, step=0.1)))\n",
    "\n",
    "        model.add(layers.Dense(5))  # Output: 5 targets\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n",
    "            ),\n",
    "            loss=\"mse\",\n",
    "            metrics=[\"mae\", \"mse\", keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Initialize the tuner\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=10,\n",
    "        executions_per_trial=2,\n",
    "        overwrite=True,\n",
    "        directory=\"tuner_dir\",\n",
    "        project_name=\"indoor_outdoor_conc\"\n",
    "    )\n",
    "\n",
    "    # Run the tuner\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    print(\"Starting hyperparameter search...\")\n",
    "    tuner.search(X_train, y_train, epochs=100, validation_split=0.2, \n",
    "                 callbacks=[early_stop], verbose=1)\n",
    "\n",
    "    # Retrieve best model and hyperparameters\n",
    "    best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "    print(\"\\nBest Hyperparameters Found:\")\n",
    "    print(best_hp.values)\n",
    "else:\n",
    "    print(\"Hyperparameter tuning skipped. Using pre-optimized parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24649e0f",
   "metadata": {},
   "source": [
    "## 6. Build and Train the Model\n",
    "\n",
    "Using the best hyperparameters found during tuning (or pre-optimized values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea20c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyperparameters (pre-optimized)\n",
    "best_hp_values = {\n",
    "    'num_layers': 3,\n",
    "    'units_0': 256,\n",
    "    'dropout_0': 0.3,\n",
    "    'units_1': 160,\n",
    "    'dropout_1': 0.1,\n",
    "    'learning_rate': 0.00047590319258532097,\n",
    "    'units_2': 160,\n",
    "    'dropout_2': 0.0\n",
    "}\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb236bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Keras Model Based on Best Hyperparameters\n",
    "model = keras.Sequential()\n",
    "\n",
    "for i in range(best_hp_values['num_layers']):\n",
    "    model.add(layers.Dense(\n",
    "        units=best_hp_values[f'units_{i}'],\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(layers.Dropout(rate=best_hp_values[f'dropout_{i}']))\n",
    "\n",
    "model.add(layers.Dense(5))  # Output: 5 targets\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=best_hp_values['learning_rate']),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\", \"mse\", keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.build(input_shape=(None, X_train.shape[1]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2794c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "print(\"Training the model...\\n\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb144d1",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "test_loss, test_mae, test_mse, test_rmse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a3993",
   "metadata": {},
   "source": [
    "## 8. Make Predictions\n",
    "\n",
    "Generate predictions and inverse transform to original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49388f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and Inverse Scale\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "print(f\"Predictions generated for {len(y_pred)} test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12248da7",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics for All Outputs\n",
    "\n",
    "Calculate detailed metrics for each output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics for All Outputs\n",
    "output_names = ['Indoor (ppm)', 'Outdoor (ppm)', 'Red Zone (miles)', \n",
    "                'Orange Zone (miles)', 'Yellow Zone (miles)']\n",
    "\n",
    "metrics_table = []\n",
    "for i, name in enumerate(output_names):\n",
    "    rmse = root_mean_squared_error(y_actual[:, i], y_pred[:, i])\n",
    "    r2 = r2_score(y_actual[:, i], y_pred[:, i])\n",
    "    mae = mean_absolute_error(y_actual[:, i], y_pred[:, i])\n",
    "    mape = (mae / np.mean(y_actual[:, i])) * 100\n",
    "\n",
    "    metrics_table.append([name, f\"{rmse:.4f}\", f\"{r2:.4f}\", f\"{mae:.4f}\", f\"{mape:.2f}%\"])\n",
    "\n",
    "table = pd.DataFrame(metrics_table, columns=[\"Output\", \"RMSE\", \"R²\", \"MAE\", \"MAPE\"])\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(tabulate.tabulate(metrics_table, headers=[\"Output\", \"RMSE\", \"R²\", \"MAE\", \"MAPE\"], \n",
    "                        tablefmt=\"github\"))\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b064c",
   "metadata": {},
   "source": [
    "## 10. Visualizations\n",
    "\n",
    "### 10.1 Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss Curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "fontfamily = {'family': 'Times New Roman', 'weight': 'regular', 'size': 14}\n",
    "plt.rc('font', **fontfamily)\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['xtick.major.size'] = 8\n",
    "plt.rcParams['xtick.minor.size'] = 4\n",
    "plt.rcParams['ytick.major.size'] = 8\n",
    "plt.rcParams['ytick.minor.size'] = 4\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['ytick.minor.width'] = 1\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='red', linewidth=2, linestyle='-.')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='blue', linewidth=2, linestyle='-.')\n",
    "\n",
    "plt.title('Model Loss Curve', weight='bold')\n",
    "plt.xlabel('Epoch', weight='bold')\n",
    "plt.ylabel(r'Loss (MSE = $\\frac{1}{n} \\sum (y_{true} - y_{pred})^2$)', weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e55162",
   "metadata": {},
   "source": [
    "### 10.2 Predicted vs Actual Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual\n",
    "plt.figure(figsize=(18, 10))\n",
    "colors = ['blue', 'green', 'purple', 'orange', 'gold']\n",
    "\n",
    "for i, name in enumerate(output_names):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.scatter(y_actual[:, i], y_pred[:, i], color=colors[i], alpha=0.6)\n",
    "    plt.plot([min(y_actual[:, i]), max(y_actual[:, i])], \n",
    "             [min(y_actual[:, i]), max(y_actual[:, i])], 'r--', linewidth=2)\n",
    "    plt.title(f'({chr(97+i)}) Predicted vs Actual for {name}', weight='bold')\n",
    "    plt.xlabel('Actual', weight='bold')\n",
    "    plt.ylabel('Predicted', weight='bold')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357bf6fa",
   "metadata": {},
   "source": [
    "### 10.3 Threat Zones Comparison Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f312f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Names and Labels\n",
    "zone_labels = output_names[2:5]  # Red, Orange, Yellow\n",
    "stats_labels = ['Min', 'Mean', 'Max']\n",
    "hatch_patterns = ['///', '...', 'xxx']\n",
    "\n",
    "# Calculate Summary Stats\n",
    "summary_actual = []\n",
    "summary_pred = []\n",
    "input_points = []\n",
    "\n",
    "X_test_Inv = scaler_X.inverse_transform(X_test)  # Inverse transform for input points\n",
    "\n",
    "for i in range(2, 5):  # Red, Orange, Yellow\n",
    "    actual = y_actual[:, i]\n",
    "    pred = y_pred[:, i]\n",
    "    summary_actual.append([np.min(actual), np.mean(actual), np.max(actual)])\n",
    "    summary_pred.append([np.min(pred), np.mean(pred), np.max(pred)])\n",
    "    \n",
    "    # Indices for min, mean (closest), and max\n",
    "    min_idx = np.argmin(actual)\n",
    "    max_idx = np.argmax(actual)\n",
    "    mean_val = np.mean(actual)\n",
    "    mean_idx = np.argmin(np.abs(actual - mean_val))\n",
    "\n",
    "    input_points.append([\n",
    "        X_test_Inv[min_idx],   # Input for min actual\n",
    "        X_test_Inv[mean_idx],  # Input for mean actual\n",
    "        X_test_Inv[max_idx]    # Input for max actual\n",
    "    ])\n",
    "\n",
    "summary_actual = np.array(summary_actual)\n",
    "summary_pred = np.array(summary_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot Setup\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bar_width = 0.5\n",
    "group_spacing = 1.3\n",
    "zone_spacing = 1\n",
    "tick_positions = []\n",
    "tick_labels = []\n",
    "\n",
    "zone_colors = [\n",
    "    ('#ff9999', '#cc0000'),  # Red Zone\n",
    "    ('#ffcc99', '#ff6600'),  # Orange Zone\n",
    "    ('#ffff99', '#cccc00'),  # Yellow Zone\n",
    "]\n",
    "\n",
    "current_pos = 0\n",
    "\n",
    "# Draw Bars\n",
    "for zone_idx, (zone, (actual_color, pred_color)) in enumerate(zip(zone_labels, zone_colors)):\n",
    "    for stat_idx, stat in enumerate(stats_labels):\n",
    "        actual_val = summary_actual[zone_idx][stat_idx]\n",
    "        pred_val = summary_pred[zone_idx][stat_idx]\n",
    "\n",
    "        actual_pos = current_pos\n",
    "        pred_pos = current_pos + bar_width\n",
    "\n",
    "        # Bar with hatch patterns\n",
    "        ax.bar(actual_pos, actual_val, width=bar_width, color=actual_color, edgecolor='black',\n",
    "               hatch=hatch_patterns[stat_idx], label=f'{stat} - Actual' if zone_idx == 0 else \"\")\n",
    "        ax.bar(pred_pos, pred_val, width=bar_width, color=pred_color, edgecolor='black',\n",
    "               hatch=hatch_patterns[stat_idx], label=f'{stat} - Predicted' if zone_idx == 0 else \"\")\n",
    "\n",
    "        # Add tick labels\n",
    "        tick_positions.append((actual_pos + pred_pos)/2)\n",
    "        if stat == 'Mean':\n",
    "            tick_labels.append(f'{stat} \\n{zone}')\n",
    "        else:\n",
    "            tick_labels.append(f'{stat}')\n",
    "\n",
    "        current_pos += group_spacing\n",
    "\n",
    "    current_pos += zone_spacing\n",
    "\n",
    "# Axis and Legend\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels(tick_labels, rotation=0, ha='center', fontsize=12, weight='bold')\n",
    "ax.set_ylabel('Distance (miles)', weight='bold')\n",
    "ax.set_title('Comparison of Predicted vs Actual Threat Zones', fontsize=14, weight='bold')\n",
    "\n",
    "# Custom legend\n",
    "import matplotlib.patches as mpatches\n",
    "legend_labels = ['Min - Actual', 'Min - Predicted', 'Mean - Actual', \n",
    "                 'Mean - Predicted', 'Max - Actual', 'Max - Predicted']\n",
    "hatches = ['///', '///', '...', '...', 'xxx', 'xxx']\n",
    "handles = [\n",
    "    mpatches.Patch(facecolor='white', edgecolor='black', hatch=h, label=l)\n",
    "    for h, l in zip(hatches, legend_labels)\n",
    "]\n",
    "ax.legend(handles=handles, ncol=3, loc='upper left', bbox_to_anchor=(0, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f554f4d",
   "metadata": {},
   "source": [
    "### 10.4 Combined Input-Output Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19484962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Table of Input Points, Actual, Predicted Summary\n",
    "combined_table = []\n",
    "for zone_idx, zone in enumerate(zone_labels):\n",
    "    for stat_idx, stat in enumerate(stats_labels):\n",
    "        inputs = input_points[zone_idx][stat_idx]\n",
    "        row = {\n",
    "            'Zone': zone,\n",
    "            'Stat': stat,\n",
    "            'Air Temperature (°C)': f\"{inputs[0]:.2f}\",\n",
    "            'Wind Speed (m/s)': f\"{inputs[1]:.2f}\",\n",
    "            'Hole Diameter (mm)': f\"{inputs[2]:.2f}\",\n",
    "            'Actual': f\"{summary_actual[zone_idx][stat_idx]:.4f}\",\n",
    "            'Predicted': f\"{summary_pred[zone_idx][stat_idx]:.4f}\"\n",
    "        }\n",
    "        combined_table.append(row)\n",
    "\n",
    "combined_df = pd.DataFrame(combined_table)\n",
    "print(\"\\nCombined Input & Output Summary Table:\")\n",
    "display(combined_df)\n",
    "\n",
    "# Save to Excel\n",
    "combined_df.to_excel(\"combined_input_output_summary.xlsx\", index=False)\n",
    "print(\"\\nTable saved to 'combined_input_output_summary.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936a3d6",
   "metadata": {},
   "source": [
    "### 10.5 Residual Plots (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32202b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Plots (Normalized)\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "for i, name in enumerate(output_names):\n",
    "    residuals = y_actual[:, i] - y_pred[:, i]\n",
    "    norm_residuals = residuals / (y_actual[:, i] + 1e-8)\n",
    "\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.scatter(y_actual[:, i], norm_residuals, color=colors[i], alpha=0.6)\n",
    "    plt.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(f'({chr(97+i)}) Normalized Residuals for {name}', weight='bold')\n",
    "    plt.xlabel('Actual Value', weight='bold')\n",
    "    plt.ylabel('Normalized Residuals', weight='bold')\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa27acb",
   "metadata": {},
   "source": [
    "## 11. Save the Model (Optional)\n",
    "\n",
    "Save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('ammonia_dispersion_model.keras')\n",
    "print(\"Model saved as 'ammonia_dispersion_model.keras'\")\n",
    "\n",
    "# Save scalers\n",
    "import pickle\n",
    "with open('scaler_X.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "with open('scaler_y.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_y, f)\n",
    "print(\"Scalers saved as 'scaler_X.pkl' and 'scaler_y.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e4fbf",
   "metadata": {},
   "source": [
    "## 12. Make Predictions on New Data (Optional)\n",
    "\n",
    "Example of how to use the trained model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5294aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict for new input\n",
    "new_data = np.array([\n",
    "    [25.0, 5.0, 10.0],  # Air Temp (°C), Wind Speed (m/s), Hole Diameter (mm)\n",
    "    [30.0, 3.0, 15.0],\n",
    "])\n",
    "\n",
    "# Scale the input\n",
    "new_data_scaled = scaler_X.transform(new_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions_scaled = model.predict(new_data_scaled)\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "\n",
    "# Display predictions\n",
    "predictions_df = pd.DataFrame(\n",
    "    predictions,\n",
    "    columns=['Indoor (ppm)', 'Outdoor (ppm)', 'Red Zone (miles)', \n",
    "             'Orange Zone (miles)', 'Yellow Zone (miles)']\n",
    ")\n",
    "print(\"\\nPredictions for new data:\")\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a54d66",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a deep learning approach for predicting ammonia dispersion and threat zones based on environmental conditions. The model shows good performance across all output variables.\n",
    "\n",
    "### Key Findings:\n",
    "- The model successfully predicts both concentration and threat zones\n",
    "- R² scores indicate strong predictive performance\n",
    "- The model can be used for safety planning and risk assessment\n",
    "\n",
    "### Future Work:\n",
    "- Include additional environmental factors\n",
    "- Test on different chemical substances\n",
    "- Deploy as a web application for real-time predictions\n",
    "\n",
    "---\n",
    "\n",
    "**Citation:** If you use this code, please cite the related research paper.\n",
    "\n",
    "**Contact:** [Add your contact information]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
